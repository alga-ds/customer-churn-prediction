# -*- coding: utf-8 -*-
"""Ml end-to-end churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JS-46yVvoSdm36I-bYPnQGk0a7dLGcve
"""

# ğŸ“¦ Install Required Libraries (for Google Colab)
!pip install shap openpyxl --quiet

# ğŸ“‚ Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import joblib

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# ğŸ“„ Load the uploaded Excel file
df = pd.read_excel('/content/Telco_customer_churn.xlsx')

import pandas as pd

file_path = "/content/Telco_customer_churn.xlsx"

# ğŸ“„ Read Excel file without setting header
df_raw = pd.read_excel(file_path, header=None)

# ğŸ‘€ Peek at the first 10 rows
df_raw.head(10)

# âœ… Set proper header from first row
df_raw.columns = df_raw.iloc[0]      # Row 0 becomes header
df = df_raw[1:].reset_index(drop=True)  # Remove old header row from data

# ğŸ§¼ Clean whitespace and fix column names
df.columns = df.columns.str.strip()

print(df.columns.tolist())

# ğŸ¯ Create binary target column
df['Churn'] = df['Churn Label'].map({'Yes': 1, 'No': 0})

# ğŸ§¹ Drop unnecessary columns
df = df.drop(columns=['Churn Label', 'CustomerID', 'Churn Reason', 'Churn Score', 'Churn Value'])

from sklearn.model_selection import train_test_split

# âœ… Split the dataset into train and test sets
X = df.drop('Churn', axis=1)
y = df['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("âœ… Shape of X_train:", X_train.shape)
print("âœ… Shape of X_test:", X_test.shape)

# ğŸ”„ Convert categorical variables to dummy/one-hot encoded variables
X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

# âœ… Align the train and test data
X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)

from sklearn.ensemble import RandomForestClassifier

# ğŸŒ² Train the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

print("âœ… Model trained successfully!")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# ğŸ§ª Predict on test set
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # For AUC

# ğŸ“ˆ Evaluate
print("âœ… Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("âœ… Classification Report:\n", classification_report(y_test, y_pred))
print("âœ… AUC Score:", roc_auc_score(y_test, y_proba))

import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ¯ Feature Importance Plot
feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)
feature_importance.nlargest(15).plot(kind='barh', figsize=(10,6), color='skyblue')
plt.title('Top 15 Feature Importances')
plt.xlabel('Importance Score')
plt.ylabel('Feature Name')
plt.gca().invert_yaxis()
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='liblinear', class_weight='balanced', max_iter=1000)

param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)
grid.fit(X_train, y_train)

# Best model and params
print("âœ… Best Parameters:", grid.best_params_)
best_model = grid.best_estimator_

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:,1]

print("âœ… Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("âœ… Classification Report:\n", classification_report(y_test, y_pred))
print("âœ… AUC Score:", roc_auc_score(y_test, y_proba))

# âœ… Convert boolean columns to integers (0 and 1)
X_train = X_train.astype(int)

# ğŸ› ï¸ Step 1: Install imbalanced-learn (if not already)
!pip install -q imbalanced-learn

# ğŸ“¦ Step 2: Import SMOTE
from imblearn.over_sampling import SMOTE

# âš™ï¸ Step 3: Apply SMOTE on the training set
smote = SMOTE(random_state=42)

X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print("Original dataset shape:", y_train.value_counts())
print("After SMOTE:", y_train_sm.value_counts())

# ğŸ¤– Step 4: Fit model on SMOTE-balanced data
model_sm = RandomForestClassifier(n_estimators=100, random_state=42)
model_sm.fit(X_train_sm, y_train_sm)

# ğŸ§ª Step 5: Predict on test set
y_pred = model_sm.predict(X_test)
y_proba = model_sm.predict_proba(X_test)[:, 1]

# ğŸ“Š Step 6: Evaluate
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

print("âœ… Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("âœ… Classification Report:\n", classification_report(y_test, y_pred))
print("âœ… AUC Score:", roc_auc_score(y_test, y_proba))

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x=y_train_sm)
plt.title("Class Distribution After SMOTE")
plt.show()

!pip install streamlit

dashboard_code = """
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

st.title("ğŸ“Š Telco Customer Churn Project Summary Dashboard")

uploaded_file = st.file_uploader("Upload the cleaned Telco churn dataset", type=["csv", "xlsx"])
if uploaded_file:
    if uploaded_file.name.endswith('.csv'):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)

    st.subheader("ğŸ—‚ï¸ Dataset Overview")
    st.write(df.head())

    st.markdown("#### ğŸ” Dataset Shape")
    st.write(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")

    st.markdown("#### ğŸ“Œ Churn Distribution")
    fig, ax = plt.subplots()
    sns.countplot(data=df, x='Churn', ax=ax)
    st.pyplot(fig)

    st.markdown("#### âš™ï¸ Features")
    st.write(df.columns.tolist())

    st.markdown("#### ğŸ”§ Preprocessing Steps Applied")
    st.markdown(\"""
    - Converted `Churn Label` to binary values
    - One-hot encoded categorical variables
    - Boolean columns converted to integers
    - Unnecessary columns like `CustomerID`, `Churn Score`, etc., dropped
    \""")

    st.markdown("#### ğŸ“‰ Model Evaluation (Example)")
    st.write({
        "Accuracy": "69.3%",
        "Precision (Churn)": "48%",
        "Recall (Churn)": "90%",
        "AUC Score": "0.839"
    })

    st.markdown("#### ğŸ“ˆ Confusion Matrix Visual")
    conf_data = [[618, 391], [42, 358]]
    fig2, ax2 = plt.subplots()
    sns.heatmap(conf_data, annot=True, fmt="d", cmap="Blues", xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
    ax2.set_xlabel("Predicted")
    ax2.set_ylabel("Actual")
    st.pyplot(fig2)

    st.markdown("#### ğŸ’¡ Improvements & Next Steps")
    st.markdown(\"""
    - Apply SMOTE inside cross-validation using `Pipeline`
    - Try XGBoost or CatBoost with `scale_pos_weight`
    - Feature engineering (charges/month, tenure bins, etc.)
    - Threshold tuning (lower from 0.5 to 0.3)
    - Use SHAP for better interpretability
    - Build Streamlit dashboard (you're doing it now ğŸ˜‰)
    \""")

    st.success("âœ¨ Dashboard loaded successfully!")
else:
    st.info("Please upload your cleaned Telco Churn dataset to begin.")
"""

with open("telco_dashboard.py", "w") as f:
    f.write(dashboard_code)

!streamlit run telco_dashboard.py & npx localtunnel --port 8501